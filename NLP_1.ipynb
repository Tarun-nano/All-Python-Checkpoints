{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"hello World, They arear is tanmay ? hsusd. Ther are ther adssse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello World, They arear is tanmay ?', 'hsusd.', 'Ther are ther adssse']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'World',\n",
       " ',',\n",
       " 'They',\n",
       " 'arear',\n",
       " 'is',\n",
       " 'tanmay',\n",
       " '?',\n",
       " 'hsusd',\n",
       " '.',\n",
       " 'Ther',\n",
       " 'are',\n",
       " 'ther',\n",
       " 'adssse']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "World\n",
      ",\n",
      "They\n",
      "arear\n",
      "is\n",
      "tanmay\n",
      "?\n",
      "hsusd\n",
      ".\n",
      "Ther\n",
      "are\n",
      "ther\n",
      "adssse\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(a):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=\"5 August 1963) is an English actor. He is best known for his roles in films, such as Prince Septimus in Stardust (2007), Lord Henry Blackwood in Sherlock Holmes (2009), Frank D'Amico in Kick-Ass (2010), Jim Prideaux in Tinker Tailor Soldier Spy (2011), George in Zero Dark Thirty (2012), Maj. Gen. Stewart Menzies in The Imitation Game (2014), Merlin in Kingsman: The Secret Service (2014) and Kingsman: The Golden Circle (2017), and Dr. Thaddeus Sivana in Shazam! (2019).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'other', 'not', \"doesn't\", 'then', 'being', 'ourselves', 'am', 'at', \"shouldn't\", \"it's\", 'nor', 'above', \"mustn't\", 'any', 'too', 'do', 'such', \"you're\", 'down', 'haven', 'did', 'after', 'of', 'now', \"needn't\", 'themselves', 'by', 'myself', 'more', 'which', 'against', 'there', 'you', 'shan', 'their', 'an', \"aren't\", 'her', 'with', 'both', 'couldn', 'don', 'should', 'hers', 'most', \"weren't\", 'had', 'who', 'over', 'up', \"you'll\", 'wouldn', 'd', 'when', 'the', 'has', 't', 'y', 'wasn', 'them', 'him', 'our', 'your', \"hasn't\", \"she's\", 'these', 'so', 'mightn', 'for', 'and', \"haven't\", \"you'd\", 'while', 'during', 'where', 'me', 'than', 'through', \"couldn't\", \"don't\", 'own', 'what', 's', 'yourselves', 'some', 'was', 'he', 'its', 'having', 'off', 'mustn', 'once', 'we', 'out', 'yours', \"didn't\", 'under', 'my', 'theirs', 'into', 'few', 'have', 'as', 'before', 'll', 'shouldn', 'if', 'no', 'yourself', 'on', 'needn', 'i', 'how', 'itself', 'about', 'are', 're', 'm', 'himself', \"shan't\", \"wasn't\", 'doing', 'those', 'or', 'been', 'his', 'is', 'ain', 'here', 'can', \"that'll\", 'ma', 'weren', 'each', 'they', 'a', 'below', 'will', 'does', 'hadn', 'whom', 'o', 'same', 'very', \"won't\", 'because', 'from', 'be', 'further', 'but', 'until', 'aren', \"should've\", 'in', 'this', \"isn't\", 'herself', \"mightn't\", \"hadn't\", 'ours', 'to', 'it', 'all', 'that', 'just', 'why', 'hasn', 'again', 'won', 'isn', 'only', 've', \"you've\", 'doesn', 'didn', \"wouldn't\", 'between', 'she', 'were'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', 'August', '1963', ')', 'English', 'actor', '.', 'He', 'best', 'known', 'roles', 'films', ',', 'Prince', 'Septimus', 'Stardust', '(', '2007', ')', ',', 'Lord', 'Henry', 'Blackwood', 'Sherlock', 'Holmes', '(', '2009', ')', ',', 'Frank', \"D'Amico\", 'Kick-Ass', '(', '2010', ')', ',', 'Jim', 'Prideaux', 'Tinker', 'Tailor', 'Soldier', 'Spy', '(', '2011', ')', ',', 'George', 'Zero', 'Dark', 'Thirty', '(', '2012', ')', ',', 'Maj.', 'Gen.', 'Stewart', 'Menzies', 'The', 'Imitation', 'Game', '(', '2014', ')', ',', 'Merlin', 'Kingsman', ':', 'The', 'Secret', 'Service', '(', '2014', ')', 'Kingsman', ':', 'The', 'Golden', 'Circle', '(', '2017', ')', ',', 'Dr.', 'Thaddeus', 'Sivana', 'Shazam', '!', '(', '2019', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "for j in words:\n",
    "    if j not in stop_words:\n",
    "        filtered_sentence.append(j)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa=\"5 August 1963) is an English actor. He is best known for his roles in films, such as Prince Septimus in Stardust (2007), Lord Henry Blackwood in Sherlock Holmes (2009), Frank D'Amico in Kick-Ass (2010), Jim Prideaux in Tinker Tailor Soldier Spy (2011), George in Zero Dark Thirty (2012), Maj. Gen. Stewart Menzies in The Imitation Game (2014), Merlin in Kingsman: The Secret Service (2014) and Kingsman: The Golden Circle (2017), and Dr. Thaddeus Si (2019).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "august\n",
      "1963\n",
      ")\n",
      "is\n",
      "an\n",
      "english\n",
      "actor\n",
      ".\n",
      "he\n",
      "is\n",
      "best\n",
      "known\n",
      "for\n",
      "hi\n",
      "role\n",
      "in\n",
      "film\n",
      ",\n",
      "such\n",
      "as\n",
      "princ\n",
      "septimu\n",
      "in\n",
      "stardust\n",
      "(\n",
      "2007\n",
      ")\n",
      ",\n",
      "lord\n",
      "henri\n",
      "blackwood\n",
      "in\n",
      "sherlock\n",
      "holm\n",
      "(\n",
      "2009\n",
      ")\n",
      ",\n",
      "frank\n",
      "d'amico\n",
      "in\n",
      "kick-ass\n",
      "(\n",
      "2010\n",
      ")\n",
      ",\n",
      "jim\n",
      "prideaux\n",
      "in\n",
      "tinker\n",
      "tailor\n",
      "soldier\n",
      "spi\n",
      "(\n",
      "2011\n",
      ")\n",
      ",\n",
      "georg\n",
      "in\n",
      "zero\n",
      "dark\n",
      "thirti\n",
      "(\n",
      "2012\n",
      ")\n",
      ",\n",
      "maj.\n",
      "gen.\n",
      "stewart\n",
      "menzi\n",
      "in\n",
      "the\n",
      "imit\n",
      "game\n",
      "(\n",
      "2014\n",
      ")\n",
      ",\n",
      "merlin\n",
      "in\n",
      "kingsman\n",
      ":\n",
      "the\n",
      "secret\n",
      "servic\n",
      "(\n",
      "2014\n",
      ")\n",
      "and\n",
      "kingsman\n",
      ":\n",
      "the\n",
      "golden\n",
      "circl\n",
      "(\n",
      "2017\n",
      ")\n",
      ",\n",
      "and\n",
      "dr.\n",
      "thaddeu\n",
      "si\n",
      "(\n",
      "2019\n",
      ")\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(ps.stem(i.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text=state_union.raw(\"2006-GWbush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=state_union.raw(\"2006-GWbush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.tokenize.punkt.PunktSentenceTokenizer at 0x1faef57b2e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_sent_tokenizer=PunktSentenceTokenizer(train_text)\n",
    "custom_sent_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized=custom_sent_tokenizer.tokenize(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words=nltk.word_tokenize(i)  \n",
    "            tagged=nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5', 'CD'), ('August', 'NNP'), ('1963', 'CD'), (')', ')'), ('is', 'VBZ'), ('an', 'DT'), ('English', 'JJ'), ('actor', 'NN'), ('.', '.')]\n",
      "[('He', 'PRP'), ('is', 'VBZ'), ('best', 'RB'), ('known', 'VBN'), ('for', 'IN'), ('his', 'PRP$'), ('roles', 'NNS'), ('in', 'IN'), ('films', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('Prince', 'NNP'), ('Septimus', 'NNP'), ('in', 'IN'), ('Stardust', 'NNP'), ('(', '('), ('2007', 'CD'), (')', ')'), (',', ','), ('Lord', 'NNP'), ('Henry', 'NNP'), ('Blackwood', 'NNP'), ('in', 'IN'), ('Sherlock', 'NNP'), ('Holmes', 'NNP'), ('(', '('), ('2009', 'CD'), (')', ')'), (',', ','), ('Frank', 'NNP'), (\"D'Amico\", 'NNP'), ('in', 'IN'), ('Kick-Ass', 'NNP'), ('(', '('), ('2010', 'CD'), (')', ')'), (',', ','), ('Jim', 'NNP'), ('Prideaux', 'NNP'), ('in', 'IN'), ('Tinker', 'NNP'), ('Tailor', 'NNP'), ('Soldier', 'NNP'), ('Spy', 'NNP'), ('(', '('), ('2011', 'CD'), (')', ')'), (',', ','), ('George', 'NNP'), ('in', 'IN'), ('Zero', 'NNP'), ('Dark', 'NNP'), ('Thirty', 'NNP'), ('(', '('), ('2012', 'CD'), (')', ')'), (',', ','), ('Maj', 'NNP'), ('.', '.')]\n",
      "[('Gen', 'NNP'), ('.', '.')]\n",
      "[('Stewart', 'NNP'), ('Menzies', 'NNP'), ('in', 'IN'), ('The', 'DT'), ('Imitation', 'NNP'), ('Game', 'NNP'), ('(', '('), ('2014', 'CD'), (')', ')'), (',', ','), ('Merlin', 'NNP'), ('in', 'IN'), ('Kingsman', 'NNP'), (':', ':'), ('The', 'DT'), ('Secret', 'NNP'), ('Service', 'NNP'), ('(', '('), ('2014', 'CD'), (')', ')'), ('and', 'CC'), ('Kingsman', 'NNP'), (':', ':'), ('The', 'DT'), ('Golden', 'NNP'), ('Circle', 'NNP'), ('(', '('), ('2017', 'CD'), (')', ')'), (',', ','), ('and', 'CC'), ('Dr', 'NNP'), ('.', '.')]\n",
      "[('Thaddeus', 'NNP'), ('Si', 'NNP'), ('(', '('), ('2019', 'CD'), (')', ')'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\"Good movie\",\"not a good movie\",\"did not like\",\"i like it\",\"Bad movie\",\"good one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvector=TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=wordvector.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.542092</td>\n",
       "      <td>0.642085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542092</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.540298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.540298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       good  good movie      like     movie       not\n",
       "0  0.542092    0.642085  0.000000  0.542092  0.000000\n",
       "1  0.456156    0.540298  0.000000  0.456156  0.540298\n",
       "2  0.000000    0.000000  0.707107  0.000000  0.707107\n",
       "3  0.000000    0.000000  1.000000  0.000000  0.000000\n",
       "4  0.000000    0.000000  0.000000  1.000000  0.000000\n",
       "5  1.000000    0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(features.todense(),columns=wordvector.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics Vidya\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'corpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2923b94bb795>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcorpora\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# Creating the term dictionary of our corpus, where every unique term is assigned an index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\corpora\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mINFO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'corpus'"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim\n",
    "from corpora import Corpus\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "generate_ngrams('this is a sample text', 2)\n",
    "# [['this', 'is'], ['is', 'a'], ['a', 'sample'], , ['sample', 'text']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# US_comments['Sentiment Scores'] = US_comments['comment_text'].apply(lambda x:sia.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
